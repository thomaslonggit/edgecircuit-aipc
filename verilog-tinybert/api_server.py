#!/usr/bin/env python3
# Copyright (C) 2024-2025 Intel
# SPDX-License-Identifier: Apache-2.0

import asyncio
import json
import queue
import threading
import time
import uuid
from typing import Dict, List, Optional, Union, AsyncGenerator
from contextlib import asynccontextmanager

import openvino_genai as ov_genai
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field


# ==== å¤ç”¨ stream.py çš„é…ç½® ====
MODEL_DIR = r".\qwen2.5-ov-int4"
DEVICE = "AUTO:NPU,GPU"
MAX_NEW_TOKENS = 640
# ===============================

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹ç®¡çº¿
pipe: Optional[ov_genai.LLMPipeline] = None


class ChatMessage(BaseModel):
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²: system, user, assistant")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")


class ChatCompletionRequest(BaseModel):
    model: str = Field(default="qwen2.5-7b-int4", description="æ¨¡å‹åç§°")
    messages: List[ChatMessage] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    temperature: Optional[float] = Field(default=0.2, ge=0, le=2, description="é‡‡æ ·æ¸©åº¦")
    max_tokens: Optional[int] = Field(default=MAX_NEW_TOKENS, gt=0, description="æœ€å¤§ç”Ÿæˆtokenæ•°")
    stream: Optional[bool] = Field(default=False, description="æ˜¯å¦æµå¼è¿”å›")
    top_p: Optional[float] = Field(default=1.0, ge=0, le=1, description="æ ¸é‡‡æ ·å‚æ•°")


class ChatCompletionChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: Optional[str] = None


class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Dict[str, int] = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}


class ChatCompletionStreamChoice(BaseModel):
    index: int
    delta: Dict[str, Optional[str]]
    finish_reason: Optional[str] = None


class ChatCompletionStreamResponse(BaseModel):
    id: str
    object: str = "chat.completion.chunk"
    created: int
    model: str
    choices: List[ChatCompletionStreamChoice]


class ModelInfo(BaseModel):
    id: str
    object: str = "model"
    created: int
    owned_by: str = "intel-openvino"


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨å¯åŠ¨å’Œå…³é—­æ—¶çš„èµ„æºç®¡ç†"""
    global pipe
    
    # å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_DIR}")
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    
    try:
        pipe = ov_genai.LLMPipeline(
            MODEL_DIR,
            DEVICE,
            PERFORMANCE_HINT="LATENCY",
            CACHE_DIR="./ov_cache"
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†èµ„æº
    if pipe:
        print("ğŸ§¹ æ¸…ç†æ¨¡å‹èµ„æº...")


# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="Intel AIPC OpenVINO GenAI API Server",
    description="åŸºäº OpenVINO GenAI çš„ OpenAI å…¼å®¹ API æœåŠ¡",
    version="1.0.0",
    lifespan=lifespan
)


def messages_to_prompt(messages: List[ChatMessage]) -> str:
    """å°† OpenAI æ ¼å¼çš„æ¶ˆæ¯è½¬æ¢ä¸ºå•ä¸ª prompt"""
    prompt_parts = []
    
    for msg in messages:
        if msg.role == "system":
            prompt_parts.append(f"System: {msg.content}")
        elif msg.role == "user":
            prompt_parts.append(f"User: {msg.content}")
        elif msg.role == "assistant":
            prompt_parts.append(f"Assistant: {msg.content}")
    
    prompt_parts.append("Assistant:")
    return "\n".join(prompt_parts)


class AsyncStreamHandler:
    """å¼‚æ­¥æµå¼å¤„ç†å™¨ï¼Œç”¨äºåœ¨åŒæ­¥çš„OpenVINOç”Ÿæˆä¸­å®ç°çœŸæ­£çš„æµå¼è¾“å‡º"""
    def __init__(self, request_id: str, created: int, model: str):
        self.request_id = request_id
        self.created = created
        self.model = model
        self.queue = queue.Queue()
        self.finished = False
    
    def stream_callback(self, subword: str) -> ov_genai.StreamingStatus:
        """OpenVINOçš„æµå¼å›è°ƒå‡½æ•°ï¼Œæ¯ç”Ÿæˆä¸€ä¸ªsubwordå°±è°ƒç”¨"""
        self.queue.put(subword)
        return ov_genai.StreamingStatus.RUNNING
    
    def finish_generation(self):
        """æ ‡è®°ç”Ÿæˆå®Œæˆ"""
        self.finished = True
        self.queue.put(None)  # å‘é€ç»“æŸä¿¡å·
    
    async def get_stream_chunks(self) -> AsyncGenerator[str, None]:
        """å¼‚æ­¥ç”Ÿæˆæµå¼å“åº”å—"""
        while True:
            try:
                # éé˜»å¡è·å–é˜Ÿåˆ—ä¸­çš„å†…å®¹
                subword = self.queue.get(timeout=0.1)
                
                if subword is None:  # ç»“æŸä¿¡å·
                    break
                
                # åˆ›å»ºæµå¼å“åº”å—
                chunk = ChatCompletionStreamResponse(
                    id=self.request_id,
                    created=self.created,
                    model=self.model,
                    choices=[
                        ChatCompletionStreamChoice(
                            index=0,
                            delta={"content": subword},
                            finish_reason=None
                        )
                    ]
                )
                yield f"data: {chunk.model_dump_json()}\n\n"
                
            except queue.Empty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œç­‰å¾…ä¸€å°æ®µæ—¶é—´
                await asyncio.sleep(0.01)
                if self.finished and self.queue.empty():
                    break
                continue
        
        # å‘é€ç»“æŸæ ‡å¿—
        final_chunk = ChatCompletionStreamResponse(
            id=self.request_id,
            created=self.created,
            model=self.model,
            choices=[
                ChatCompletionStreamChoice(
                    index=0,
                    delta={},
                    finish_reason="stop"
                )
            ]
        )
        yield f"data: {final_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"


def run_generation_in_thread(pipe, prompt, gen_cfg, stream_handler):
    """åœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œç”Ÿæˆï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹"""
    try:
        pipe.start_chat()
        pipe.generate(prompt, gen_cfg, stream_handler.stream_callback)
    finally:
        pipe.finish_chat()
        stream_handler.finish_generation()


async def generate_stream_response(
    request: ChatCompletionRequest,
    request_id: str,
    created: int
) -> AsyncGenerator[str, None]:
    """ç”Ÿæˆæµå¼å“åº”"""
    global pipe
    
    if not pipe:
        raise HTTPException(status_code=500, detail="æ¨¡å‹æœªåŠ è½½")
    
    # é…ç½®ç”Ÿæˆå‚æ•°
    gen_cfg = ov_genai.GenerationConfig(
        max_new_tokens=request.max_tokens,
        temperature=request.temperature,
        top_p=request.top_p,
    )
    
    # è½¬æ¢æ¶ˆæ¯ä¸º prompt
    prompt = messages_to_prompt(request.messages)
    
    # åˆ›å»ºå¼‚æ­¥æµå¼å¤„ç†å™¨
    stream_handler = AsyncStreamHandler(request_id, created, request.model)
    
    # åœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œç”Ÿæˆï¼Œé¿å…é˜»å¡
    generation_thread = threading.Thread(
        target=run_generation_in_thread,
        args=(pipe, prompt, gen_cfg, stream_handler)
    )
    generation_thread.start()
    
    # å¼‚æ­¥ç”Ÿæˆæµå¼å“åº”
    async for chunk in stream_handler.get_stream_chunks():
        yield chunk
    
    # ç­‰å¾…ç”Ÿæˆçº¿ç¨‹å®Œæˆ
    generation_thread.join()


@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest):
    """åˆ›å»ºèŠå¤©å®Œæˆ (å…¼å®¹ OpenAI API)"""
    global pipe
    
    if not pipe:
        raise HTTPException(status_code=500, detail="æ¨¡å‹æœªåŠ è½½")
    
    # ç”Ÿæˆè¯·æ±‚IDå’Œæ—¶é—´æˆ³
    request_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
    created = int(time.time())
    
    try:
        if request.stream:
            # æµå¼å“åº”
            return StreamingResponse(
                generate_stream_response(request, request_id, created),
                media_type="text/plain",
                headers={"Cache-Control": "no-cache"}
            )
        else:
            # éæµå¼å“åº”
            gen_cfg = ov_genai.GenerationConfig(
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
            )
            
            prompt = messages_to_prompt(request.messages)
            
            # ç”Ÿæˆå“åº”
            pipe.start_chat()
            try:
                result = pipe.generate(prompt, gen_cfg)
                response_content = result
            finally:
                pipe.finish_chat()
            
            # æ„é€ å“åº”
            response = ChatCompletionResponse(
                id=request_id,
                created=created,
                model=request.model,
                choices=[
                    ChatCompletionChoice(
                        index=0,
                        message=ChatMessage(role="assistant", content=response_content),
                        finish_reason="stop"
                    )
                ]
            )
            
            return response
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå¤±è´¥: {str(e)}")


@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return {
        "object": "list",
        "data": [
            ModelInfo(
                id="qwen2.5-7b-int4",
                created=int(time.time()),
                owned_by="intel-openvino"
            )
        ]
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    global pipe
    return {
        "status": "healthy" if pipe else "unhealthy",
        "model_loaded": pipe is not None,
        "device": DEVICE,
        "model_dir": MODEL_DIR
    }


@app.get("/")
async def root():
    """æ ¹è·¯å¾„ä¿¡æ¯"""
    return {
        "message": "Intel AIPC OpenVINO GenAI API Server",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health"
    }


if __name__ == "__main__":
    import uvicorn
    
    print("ğŸš€ å¯åŠ¨ Intel AIPC OpenVINO GenAI API æœåŠ¡å™¨...")
    print("ğŸ“– API æ–‡æ¡£: http://localhost:8000/docs")
    print("ğŸ¥ å¥åº·æ£€æŸ¥: http://localhost:8000/health")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    ) 