#!/usr/bin/env python3
# Copyright (C) 2024-2025 Intel
# SPDX-License-Identifier: Apache-2.0

import openvino_genai as ov_genai

# ==== è¯·æ ¹æ®éœ€è¦ä¿®æ”¹è¿™ 3 è¡Œ ====
MODEL_DIR      = r".\qwen2.5-ov-int4"      # æœ¬åœ° INT4-OV ç›®å½•æˆ– HF ID
DEVICE         = "AUTO:NPU,GPU"            # å…ˆ NPUï¼Œä¸è¡Œå°± GPU
MAX_NEW_TOKENS = 640                       # æ¯è½®æœ€å¤šç”Ÿæˆ token æ•°
# =================================

def streamer(subword: str) -> ov_genai.StreamingStatus:
    """æ¯æ”¶åˆ°ä¸€ä¸ª sub-word å°±ç«‹å³æ‰“å°ã€‚"""
    print(subword, end='', flush=True)
    return ov_genai.StreamingStatus.RUNNING   # ä¸æ‰“æ–­ç”Ÿæˆ

def main():
    # 1) åˆå§‹åŒ–ç®¡çº¿
    pipe = ov_genai.LLMPipeline(
        MODEL_DIR,
        DEVICE,
        PERFORMANCE_HINT="LATENCY",
        CACHE_DIR="./ov_cache"               # å¯é€‰ï¼šKV-cache æ”¾ SSD
    )

    # 2) é…ç½®ç”Ÿæˆå‚æ•°
    gen_cfg = ov_genai.GenerationConfig(
        max_new_tokens=MAX_NEW_TOKENS,
        temperature=0.2,                     # ä½ æƒ³æ”¹åˆ«çš„é‡‡æ ·å‚æ•°ï¼Œç›´æ¥åŠ 
    )

    # 3) è¿›å…¥å¤šè½®å¯¹è¯å¾ªç¯
    pipe.start_chat()                       # åˆ›å»ºä¼šè¯å¹¶å¯ç”¨ KV-cache
    try:
        while True:
            try:
                prompt = input("\nğŸŸ¢ ä½ ï¼š")
            except (EOFError, KeyboardInterrupt):
                break

            print("ğŸ”µ åŠ©æ‰‹ï¼š", end='', flush=True)
            pipe.generate(prompt, gen_cfg, streamer)   # â† æµå¼å›è°ƒ
            print("\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    finally:
        pipe.finish_chat()                  # æ¸…ç†èµ„æº

if __name__ == "__main__":
    main()
