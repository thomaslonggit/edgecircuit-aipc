#!/usr/bin/env python3
# Copyright (C) 2024-2025 Intel
# SPDX-License-Identifier: Apache-2.0

import openvino_genai as ov_genai
import time
import statistics
import json
from typing import List, Dict, Tuple
from datetime import datetime
import argparse
import sys

# ==== é…ç½®å‚æ•° ====
MODEL_DIR = r".\qwen2.5-ov-int4"
CACHE_DIR = "./ov_cache"

# æµ‹è¯•é…ç½®
TEST_PROMPTS = [
    "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å’Œæœªæ¥è¶‹åŠ¿ã€‚",
    "è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Œä»¥åŠå®ƒåœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨ã€‚",
    "æè¿°ä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†å’Œæ½œåœ¨åº”ç”¨åœºæ™¯ã€‚",
    "åˆ†æåŒºå—é“¾æŠ€æœ¯çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚",
    "è¯·å†™ä¸€æ®µå…³äºç¯å¢ƒä¿æŠ¤é‡è¦æ€§çš„æ–‡ç« ã€‚",
    "è§£é‡Šæœºå™¨å­¦ä¹ ä¸­çš„ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„åŒºåˆ«ã€‚",
    "ä»‹ç»äº‘è®¡ç®—çš„æœåŠ¡æ¨¡å¼å’Œéƒ¨ç½²æ¨¡å¼ã€‚",
    "åˆ†æ5GæŠ€æœ¯å¯¹ç‰©è”ç½‘å‘å±•çš„æ¨åŠ¨ä½œç”¨ã€‚"
]

DEVICES = ["CPU", "GPU", "NPU"]
MAX_NEW_TOKENS = 512
WARMUP_ROUNDS = 2
TEST_ROUNDS = 5

class TokenCounter:
    """Tokenè®¡æ•°å™¨ï¼Œç”¨äºç»Ÿè®¡ç”Ÿæˆçš„tokenæ•°é‡"""
    def __init__(self):
        self.count = 0
    
    def __call__(self, subword: str) -> ov_genai.StreamingStatus:
        self.count += 1
        return ov_genai.StreamingStatus.RUNNING

class BenchmarkResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    def __init__(self):
        self.device = ""
        self.prompt_index = 0
        self.prompt = ""
        self.tokens_generated = 0
        self.time_taken = 0.0
        self.tokens_per_second = 0.0
        self.first_token_latency = 0.0

class DeviceBenchmark:
    """è®¾å¤‡æ€§èƒ½æµ‹è¯•ç±»"""
    
    def __init__(self, device: str):
        self.device = device
        self.results: List[BenchmarkResult] = []
        
    def setup_pipeline(self) -> ov_genai.LLMPipeline:
        """åˆå§‹åŒ–æ¨ç†ç®¡çº¿"""
        try:
            print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ– {self.device} è®¾å¤‡...")
            
            # æ ¹æ®è®¾å¤‡é€‰æ‹©ä¸åŒçš„é…ç½®
            if self.device == "CPU":
                device_str = "CPU"
                hint = "THROUGHPUT"
            elif self.device == "GPU":
                device_str = "GPU"
                hint = "THROUGHPUT"
            elif self.device == "NPU":
                device_str = "NPU"
                hint = "LATENCY"
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è®¾å¤‡: {self.device}")
            
            pipe = ov_genai.LLMPipeline(
                MODEL_DIR,
                device_str,
                PERFORMANCE_HINT=hint,
                CACHE_DIR=CACHE_DIR
            )
            
            print(f"âœ… {self.device} è®¾å¤‡åˆå§‹åŒ–æˆåŠŸ")
            return pipe
            
        except Exception as e:
            print(f"âŒ {self.device} è®¾å¤‡åˆå§‹åŒ–å¤±è´¥: {e}")
            return None
    
    def run_single_test(self, pipe: ov_genai.LLMPipeline, prompt: str, prompt_index: int) -> BenchmarkResult:
        """è¿è¡Œå•æ¬¡æµ‹è¯•"""
        result = BenchmarkResult()
        result.device = self.device
        result.prompt_index = prompt_index
        result.prompt = prompt
        
        # åˆ›å»ºtokenè®¡æ•°å™¨
        token_counter = TokenCounter()
        
        # ç”Ÿæˆé…ç½®
        gen_cfg = ov_genai.GenerationConfig(
            max_new_tokens=MAX_NEW_TOKENS,
            temperature=0.1,  # ä½æ¸©åº¦ä¿è¯ç»“æœä¸€è‡´æ€§
            do_sample=False   # ç¡®å®šæ€§ç”Ÿæˆ
        )
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.perf_counter()
        first_token_time = None
        
        def streaming_callback(subword: str) -> ov_genai.StreamingStatus:
            nonlocal first_token_time
            if first_token_time is None:
                first_token_time = time.perf_counter()
            return token_counter(subword)
        
        # æ‰§è¡Œæ¨ç†
        try:
            pipe.generate(prompt, gen_cfg, streaming_callback)
            end_time = time.perf_counter()
            
            # è®¡ç®—ç»“æœ
            result.tokens_generated = token_counter.count
            result.time_taken = end_time - start_time
            result.first_token_latency = (first_token_time - start_time) if first_token_time else 0
            result.tokens_per_second = result.tokens_generated / result.time_taken if result.time_taken > 0 else 0
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            result.tokens_per_second = 0
            
        return result
    
    def run_warmup(self, pipe: ov_genai.LLMPipeline):
        """è¿è¡Œé¢„çƒ­æµ‹è¯•"""
        print(f"ğŸ”¥ {self.device} é¢„çƒ­ä¸­...")
        for i in range(WARMUP_ROUNDS):
            prompt = TEST_PROMPTS[i % len(TEST_PROMPTS)]
            self.run_single_test(pipe, prompt, -1)  # é¢„çƒ­ä¸è®°å½•ç»“æœ
        print(f"âœ… {self.device} é¢„çƒ­å®Œæˆ")
    
    def run_benchmark(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹æµ‹è¯• {self.device} è®¾å¤‡æ€§èƒ½")
        print(f"{'='*60}")
        
        # åˆå§‹åŒ–ç®¡çº¿
        pipe = self.setup_pipeline()
        if pipe is None:
            return False
        
        try:
            # é¢„çƒ­
            self.run_warmup(pipe)
            
            # æ­£å¼æµ‹è¯•
            print(f"ğŸ“Š å¼€å§‹æ­£å¼æµ‹è¯•...")
            for round_num in range(TEST_ROUNDS):
                print(f"  è½®æ¬¡ {round_num + 1}/{TEST_ROUNDS}")
                
                for prompt_idx, prompt in enumerate(TEST_PROMPTS):
                    print(f"    æµ‹è¯•ç”¨ä¾‹ {prompt_idx + 1}/{len(TEST_PROMPTS)}: {prompt[:30]}...")
                    
                    result = self.run_single_test(pipe, prompt, prompt_idx)
                    self.results.append(result)
                    
                    print(f"      ç”Ÿæˆ {result.tokens_generated} tokens, "
                          f"ç”¨æ—¶ {result.time_taken:.2f}s, "
                          f"é€Ÿåº¦ {result.tokens_per_second:.2f} tokens/s")
            
            print(f"âœ… {self.device} æµ‹è¯•å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ {self.device} æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
        finally:
            # æ¸…ç†èµ„æº
            del pipe
    
    def get_statistics(self) -> Dict:
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        if not self.results:
            return {}
        
        throughputs = [r.tokens_per_second for r in self.results if r.tokens_per_second > 0]
        latencies = [r.first_token_latency for r in self.results if r.first_token_latency > 0]
        
        if not throughputs:
            return {}
        
        return {
            "device": self.device,
            "total_tests": len(self.results),
            "successful_tests": len(throughputs),
            "throughput": {
                "mean": statistics.mean(throughputs),
                "median": statistics.median(throughputs),
                "std": statistics.stdev(throughputs) if len(throughputs) > 1 else 0,
                "min": min(throughputs),
                "max": max(throughputs)
            },
            "first_token_latency": {
                "mean": statistics.mean(latencies) if latencies else 0,
                "median": statistics.median(latencies) if latencies else 0,
                "std": statistics.stdev(latencies) if len(latencies) > 1 else 0,
                "min": min(latencies) if latencies else 0,
                "max": max(latencies) if latencies else 0
            }
        }

class BenchmarkRunner:
    """åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, devices: List[str] = None):
        self.devices = devices or DEVICES
        self.device_benchmarks: Dict[str, DeviceBenchmark] = {}
        self.start_time = None
        self.end_time = None
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰è®¾å¤‡çš„æµ‹è¯•"""
        print("ğŸ¯ Intel AIPC æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ”§ æ¨¡å‹è·¯å¾„: {MODEL_DIR}")
        print(f"ğŸ›ï¸  æœ€å¤§tokenæ•°: {MAX_NEW_TOKENS}")
        print(f"ğŸ”„ é¢„çƒ­è½®æ¬¡: {WARMUP_ROUNDS}")
        print(f"ğŸ“Š æµ‹è¯•è½®æ¬¡: {TEST_ROUNDS}")
        print(f"ğŸ“ æµ‹è¯•ç”¨ä¾‹æ•°: {len(TEST_PROMPTS)}")
        
        self.start_time = time.time()
        
        for device in self.devices:
            benchmark = DeviceBenchmark(device)
            success = benchmark.run_benchmark()
            
            if success:
                self.device_benchmarks[device] = benchmark
            else:
                print(f"âš ï¸  {device} è®¾å¤‡æµ‹è¯•è·³è¿‡")
        
        self.end_time = time.time()
    
    def generate_report(self) -> Dict:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = {
            "test_info": {
                "timestamp": datetime.now().isoformat(),
                "model_path": MODEL_DIR,
                "max_new_tokens": MAX_NEW_TOKENS,
                "warmup_rounds": WARMUP_ROUNDS,
                "test_rounds": TEST_ROUNDS,
                "test_prompts_count": len(TEST_PROMPTS),
                "total_duration": self.end_time - self.start_time if self.end_time and self.start_time else 0
            },
            "device_results": {}
        }
        
        for device, benchmark in self.device_benchmarks.items():
            stats = benchmark.get_statistics()
            if stats:
                report["device_results"][device] = stats
        
        return report
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print(f"\n{'='*80}")
        print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
        print(f"{'='*80}")
        
        if not self.device_benchmarks:
            print("âŒ æ²¡æœ‰æˆåŠŸå®Œæˆçš„æµ‹è¯•")
            return
        
        # è®¾å¤‡æ€§èƒ½æ’åº
        device_performance = []
        for device, benchmark in self.device_benchmarks.items():
            stats = benchmark.get_statistics()
            if stats and stats.get("throughput"):
                device_performance.append((device, stats["throughput"]["mean"]))
        
        device_performance.sort(key=lambda x: x[1], reverse=True)
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print(f"{'è®¾å¤‡':<8} {'å¹³å‡ååç‡':<15} {'ä¸­ä½æ•°ååç‡':<15} {'é¦–tokenå»¶è¿Ÿ':<15} {'æ ‡å‡†å·®':<10}")
        print("-" * 80)
        
        for device, benchmark in self.device_benchmarks.items():
            stats = benchmark.get_statistics()
            if stats and stats.get("throughput"):
                tp = stats["throughput"]
                lat = stats["first_token_latency"]
                print(f"{device:<8} {tp['mean']:<15.2f} {tp['median']:<15.2f} "
                      f"{lat['mean']*1000:<15.1f} {tp['std']:<10.2f}")
        
        # æ¨èæ–¹æ¡ˆ
        print(f"\nğŸ† æ€§èƒ½æ’è¡Œæ¦œ:")
        for i, (device, throughput) in enumerate(device_performance, 1):
            print(f"  {i}. {device}: {throughput:.2f} tokens/s")
        
        if device_performance:
            best_device = device_performance[0][0]
            print(f"\nğŸ’¡ æ¨èæ–¹æ¡ˆ: {best_device} (æœ€é«˜ååç‡: {device_performance[0][1]:.2f} tokens/s)")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report = self.generate_report()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"benchmark_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

def main():
    parser = argparse.ArgumentParser(description='Intel AIPC æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•')
    parser.add_argument('--devices', nargs='+', choices=['CPU', 'GPU', 'NPU'], 
                       default=['CPU', 'GPU', 'NPU'], help='è¦æµ‹è¯•çš„è®¾å¤‡åˆ—è¡¨')
    parser.add_argument('--tokens', type=int, default=512, help='æœ€å¤§ç”Ÿæˆtokenæ•°')
    parser.add_argument('--rounds', type=int, default=5, help='æµ‹è¯•è½®æ¬¡')
    
    args = parser.parse_args()
    
    global MAX_NEW_TOKENS, TEST_ROUNDS
    MAX_NEW_TOKENS = args.tokens
    TEST_ROUNDS = args.rounds
    
    try:
        runner = BenchmarkRunner(args.devices)
        runner.run_all_tests()
        runner.print_summary()
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 